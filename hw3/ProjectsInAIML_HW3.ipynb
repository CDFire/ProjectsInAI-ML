{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7k9ANJsiLrM/fepuvBbiJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CDFire/ProjectsInAI-ML/blob/main/hw3/ProjectsInAIML_HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Part 1"
      ],
      "metadata": {
        "id": "j_Mt_pWPr5WM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset:** https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\n",
        "\n",
        "I would like to build a binary classifier that can accurately detect fraudulent transactions."
      ],
      "metadata": {
        "id": "vg7l3HDL3MnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "5lIDKvHS1x7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfZDU_P0r0DC"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork:\n",
        "  # Initializes neural network with random weights and zero biases.\n",
        "  def __init__(self, input_size, hidden_sizes, output_size, learning_rate, n_iter):\n",
        "    # If hidden_sizes is integer, convert it to a list.\n",
        "    if isinstance(hidden_sizes, int):\n",
        "      hidden_sizes = [hidden_sizes]\n",
        "    self.input_size = input_size\n",
        "    self.hidden_sizes = hidden_sizes\n",
        "    self.output_size = output_size\n",
        "    self.learning_rate = learning_rate\n",
        "    self.n_iter = n_iter\n",
        "\n",
        "    # Initialize weights and biases for each layer.\n",
        "    self.weights = []\n",
        "    self.biases = []\n",
        "\n",
        "    # First hidden layer\n",
        "    self.weights.append(np.random.randn(input_size, hidden_sizes[0]) * np.sqrt(2. / input_size))\n",
        "    self.biases.append(np.zeros((1, hidden_sizes[0])))\n",
        "\n",
        "    # Additional hidden layers\n",
        "    for i in range(1, len(hidden_sizes)):\n",
        "      self.weights.append(np.random.randn(hidden_sizes[i-1], hidden_sizes[i]) * np.sqrt(2. / hidden_sizes[i-1]))\n",
        "      self.biases.append(np.zeros((1, hidden_sizes[i])))\n",
        "\n",
        "    # Output layer\n",
        "    self.weights.append(np.random.randn(hidden_sizes[-1], output_size) * np.sqrt(2. / hidden_sizes[-1]))\n",
        "    self.biases.append(np.zeros((1, output_size)))\n",
        "\n",
        "  # Sigmoid activation function\n",
        "  def sigmoid(self, z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "  # Sigmoid derivative\n",
        "  def sigmoid_derivative(self, z):\n",
        "    return z * (1 - z)\n",
        "\n",
        "  # ReLU activation function\n",
        "  def relu(self, z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "  # ReLU derivative\n",
        "  def relu_derivative(self, z):\n",
        "    dz = np.array(z)\n",
        "    dz[z <= 0] = 0\n",
        "    dz[z > 0] = 1\n",
        "    return dz\n",
        "\n",
        "  # Forward propagation\n",
        "  def forward(self, X):\n",
        "    # Lists to store linear combinations and activations for each layer.\n",
        "    self.Zs = []\n",
        "    self.As = [X]\n",
        "\n",
        "    # Loop through each layer.\n",
        "    for i in range(len(self.weights)):\n",
        "      # Hidden layer linear step\n",
        "      z = np.dot(self.As[i], self.weights[i]) + self.biases[i]\n",
        "      self.Zs.append(z)\n",
        "      # For hidden layers, use ReLU activation. For the output layer, use sigmoid.\n",
        "      if i == len(self.weights) - 1:\n",
        "        a = self.sigmoid(z)\n",
        "      else:\n",
        "        a = self.relu(z)\n",
        "      self.As.append(a)\n",
        "\n",
        "    return self.As[-1]\n",
        "\n",
        "  # Cost function\n",
        "  def cost(self, y, output):\n",
        "    m = y.shape[0]\n",
        "    epsilon = 1e-8\n",
        "    cost = -np.sum(y * np.log(output + epsilon) + (1 - y) * np.log(1 - output + epsilon)) / m\n",
        "    return cost\n",
        "\n",
        "  # Backward propagation\n",
        "  def backward(self, y):\n",
        "    m = y.shape[0]\n",
        "    L = len(self.weights)\n",
        "    dZ = [None] * L\n",
        "    dW = [None] * L\n",
        "    db = [None] * L\n",
        "\n",
        "    # Calculate the gradient for the output layer.\n",
        "    dZ[L-1] = self.As[-1] - y\n",
        "    dW[L-1] = np.dot(self.As[L-1].T, dZ[L-1]) / m\n",
        "    db[L-1] = np.sum(dZ[L-1], axis=0, keepdims=True) / m\n",
        "\n",
        "    # Backpropagate error to hidden layers.\n",
        "    for i in range(L-2, -1, -1):\n",
        "      dA = np.dot(dZ[i+1], self.weights[i+1].T)\n",
        "      dZ[i] = dA * self.relu_derivative(self.Zs[i])\n",
        "      dW[i] = np.dot(self.As[i].T, dZ[i]) / m\n",
        "      db[i] = np.sum(dZ[i], axis=0, keepdims=True) / m\n",
        "\n",
        "    # Update weights and biases with gradient descent.\n",
        "    for i in range(L):\n",
        "      self.weights[i] -= self.learning_rate * dW[i]\n",
        "      self.biases[i]  -= self.learning_rate * db[i]\n",
        "\n",
        "  # Train model\n",
        "  def train(self, X, y):\n",
        "    for i in range(self.n_iter):\n",
        "      # Forward propagation\n",
        "      output = self.forward(X)\n",
        "\n",
        "      # Output cost updates\n",
        "      if i % 100 == 0:\n",
        "        cost_value = self.cost(y, output)\n",
        "        print(f\"Iteration {i}: cost = {cost_value:.4f}\")\n",
        "\n",
        "      # Backward propagation\n",
        "      self.backward(y)\n",
        "\n",
        "  # Predict X\n",
        "  def predict(self, X):\n",
        "    # Forward propagation.\n",
        "    output = self.forward(X)\n",
        "    # Convert probabilities into binary class labels\n",
        "    predictions = (output > 0.5).astype(int)\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "data = pd.read_csv('creditcard.csv')\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Split dataset into data and labels\n",
        "X = data.drop('Class', axis=1)\n",
        "y = data['Class']\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split into train and test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, stratify=y)\n",
        "\n",
        "# Convert to 2D numpy array for neural network use\n",
        "y_train = y_train.values.reshape(-1, 1)\n",
        "y_test = y_test.values.reshape(-1, 1)"
      ],
      "metadata": {
        "id": "9bTvYU94zeii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create neural network\n",
        "nn = NeuralNetwork(input_size=X_train.shape[1], hidden_sizes=[16, 8], output_size=1, learning_rate=0.001, n_iter=5000)\n",
        "\n",
        "# Train the neural network\n",
        "nn.train(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNvEkMDG5deE",
        "outputId": "f2923c8d-7c59-4887-e5f4-4eb16f98a8bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: cost = 0.9820\n",
            "Iteration 100: cost = 0.7341\n",
            "Iteration 200: cost = 0.5773\n",
            "Iteration 300: cost = 0.4718\n",
            "Iteration 400: cost = 0.3971\n",
            "Iteration 500: cost = 0.3415\n",
            "Iteration 600: cost = 0.2987\n",
            "Iteration 700: cost = 0.2647\n",
            "Iteration 800: cost = 0.2369\n",
            "Iteration 900: cost = 0.2139\n",
            "Iteration 1000: cost = 0.1946\n",
            "Iteration 1100: cost = 0.1781\n",
            "Iteration 1200: cost = 0.1639\n",
            "Iteration 1300: cost = 0.1516\n",
            "Iteration 1400: cost = 0.1408\n",
            "Iteration 1500: cost = 0.1313\n",
            "Iteration 1600: cost = 0.1229\n",
            "Iteration 1700: cost = 0.1155\n",
            "Iteration 1800: cost = 0.1088\n",
            "Iteration 1900: cost = 0.1028\n",
            "Iteration 2000: cost = 0.0974\n",
            "Iteration 2100: cost = 0.0926\n",
            "Iteration 2200: cost = 0.0882\n",
            "Iteration 2300: cost = 0.0841\n",
            "Iteration 2400: cost = 0.0805\n",
            "Iteration 2500: cost = 0.0771\n",
            "Iteration 2600: cost = 0.0740\n",
            "Iteration 2700: cost = 0.0712\n",
            "Iteration 2800: cost = 0.0686\n",
            "Iteration 2900: cost = 0.0662\n",
            "Iteration 3000: cost = 0.0639\n",
            "Iteration 3100: cost = 0.0619\n",
            "Iteration 3200: cost = 0.0599\n",
            "Iteration 3300: cost = 0.0581\n",
            "Iteration 3400: cost = 0.0565\n",
            "Iteration 3500: cost = 0.0549\n",
            "Iteration 3600: cost = 0.0534\n",
            "Iteration 3700: cost = 0.0521\n",
            "Iteration 3800: cost = 0.0508\n",
            "Iteration 3900: cost = 0.0496\n",
            "Iteration 4000: cost = 0.0484\n",
            "Iteration 4100: cost = 0.0473\n",
            "Iteration 4200: cost = 0.0463\n",
            "Iteration 4300: cost = 0.0453\n",
            "Iteration 4400: cost = 0.0444\n",
            "Iteration 4500: cost = 0.0435\n",
            "Iteration 4600: cost = 0.0427\n",
            "Iteration 4700: cost = 0.0419\n",
            "Iteration 4800: cost = 0.0412\n",
            "Iteration 4900: cost = 0.0404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the end, I decided to stick with my original gradient descent function. I implemented mini-batch gradient descent to see if it would help, but it didn't have a huge impact on the runtime and overall just wasn't much different. The neural network didn't take too long to train in the first place ~10-20 minutes so I just stuck with my original implementation."
      ],
      "metadata": {
        "id": "wgEzHkNgAoGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "predictions = nn.predict(X_test)\n",
        "accuracy = np.mean(predictions == y_test)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4-de3wu8yvC",
        "outputId": "a3a39e0e-5619-49d8-ba71-d297c76a05c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9982795547909132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2"
      ],
      "metadata": {
        "id": "QL_S4LWxBr2U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the resources I used and why they were essential:\n",
        "1. PyTorch Tutorials: Build a Model from Scratch: https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html\n",
        "\n",
        "  I have worked with pytorch a few times, but even so, I find myself going back to this tutorial when making a new model. This resource is invaluable for understanding how to structure neural networks and keep track of computations.\n",
        "\n",
        "2. Deep Learning with PyTorch: A 60 Minute Blitz: https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n",
        "\n",
        "  This video is a great comprehensive overview of building neural networks in PyTorch. It reinforces the concepts from the first resource and gave me practical examples on how to implement a simple 2-layer network, including loss computation and optimization."
      ],
      "metadata": {
        "id": "6aLE1KI-CDjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "fi1sGyhK7hZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"creditcard.csv\")"
      ],
      "metadata": {
        "id": "LGW2B-6_QeeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exploratory Data Analysis:**"
      ],
      "metadata": {
        "id": "HEg51oKMQ_v2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove rows with Na values\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Show distribution of classes\n",
        "sns.countplot(x=\"Class\", data=data)\n",
        "plt.title(\"Distribution of Transaction Classes (0: Non-Fraud, 1: Fraud)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "q_-43vmJQldn",
        "outputId": "0bf66572-9599-4474-8cff-d59fc29fe7e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**train-dev-test split:**"
      ],
      "metadata": {
        "id": "I3xfTcjkRspw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data:\n",
        "# Use 20% as test data.\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X_scaled, y, test_size=0.2, stratify=y)\n",
        "\n",
        "# From remaining 80%, use 20% as a development set.\n",
        "X_train, X_dev, y_train, y_dev = train_test_split(X_temp, y_temp, test_size=0.2, stratify=y_temp)\n",
        "\n",
        "# Convert data to pytorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
        "X_dev = torch.tensor(X_dev, dtype=torch.float32)\n",
        "y_dev = torch.tensor(y_dev.values, dtype=torch.float32).view(-1, 1)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)"
      ],
      "metadata": {
        "id": "sbzqVpXNRRMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**forward propagation:**"
      ],
      "metadata": {
        "id": "aO_JBJOqR1Bu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoLayerNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(TwoLayerNN, self).__init__()\n",
        "        # First fully connected layer\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        # ReLU activation for the hidden layer\n",
        "        self.relu = nn.ReLU()\n",
        "        # Second fully connected layer\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        # Sigmoid activation for the output layer (for binary classification)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.sigmoid(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "epJgwHmZRyu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper-parameters\n",
        "input_dim = X_train.shape[1]   # number of features: 30\n",
        "hidden_dim = 16                # hidden layer size of 16\n",
        "output_dim = 1\n",
        "num_epochs = 10\n",
        "batch_size = 128\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Instantiate the model with chosen hyperparameters\n",
        "model = TwoLayerNN(input_dim, hidden_dim, output_dim)"
      ],
      "metadata": {
        "id": "8gb3R0LBSXDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**cost function**"
      ],
      "metadata": {
        "id": "Pd_vmBmhS2Ip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use binary cross entropy loss for the cost function\n",
        "criterion = nn.BCELoss()"
      ],
      "metadata": {
        "id": "7tGYftBTS0_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement gradient descent"
      ],
      "metadata": {
        "id": "escrrEuzTHJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Adam optimizer for model training\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Create DataLoader for mini-batch training\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "lGRwpaTtTEzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        # Forward propagation\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        # Backward propagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * batch_X.size(0)\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "    # Evaluate on the development set at each epoch\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        dev_outputs = model(X_dev)\n",
        "        dev_loss = criterion(dev_outputs, y_dev).item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Dev Loss: {dev_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qmqb3yPKThZV",
        "outputId": "adb14f7e-cd62-484a-da09-2192f4804ff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Train Loss: 0.0798, Dev Loss: 0.0053\n",
            "Epoch [2/10], Train Loss: 0.0046, Dev Loss: 0.0034\n",
            "Epoch [3/10], Train Loss: 0.0036, Dev Loss: 0.0031\n",
            "Epoch [4/10], Train Loss: 0.0032, Dev Loss: 0.0029\n",
            "Epoch [5/10], Train Loss: 0.0030, Dev Loss: 0.0028\n",
            "Epoch [6/10], Train Loss: 0.0029, Dev Loss: 0.0029\n",
            "Epoch [7/10], Train Loss: 0.0028, Dev Loss: 0.0028\n",
            "Epoch [8/10], Train Loss: 0.0026, Dev Loss: 0.0030\n",
            "Epoch [9/10], Train Loss: 0.0026, Dev Loss: 0.0028\n",
            "Epoch [10/10], Train Loss: 0.0025, Dev Loss: 0.0028\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**results:**"
      ],
      "metadata": {
        "id": "PmKj4iA_T_xR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test)\n",
        "    # Convert probabilities to binary predictions\n",
        "    test_predictions = (test_outputs >= 0.5).float()\n",
        "\n",
        "    # Calculate accuracy\n",
        "    test_accuracy = (test_predictions.eq(y_test).sum().item()) / y_test.shape[0]\n",
        "    print(\"\\nTest Accuracy:\", test_accuracy)\n",
        "\n",
        "    # Convert predictions and labels to NumPy arrays for metric calculations\n",
        "    y_test_np = y_test.cpu().numpy()\n",
        "    test_predictions_np = test_predictions.cpu().numpy()\n",
        "\n",
        "    # Calculate Precision, Recall, and F1-score\n",
        "    precision = precision_score(y_test_np, test_predictions_np)\n",
        "    recall = recall_score(y_test_np, test_predictions_np)\n",
        "    f1 = f1_score(y_test_np, test_predictions_np)\n",
        "\n",
        "    print(\"Test Precision: {:.4f}\".format(precision))\n",
        "    print(\"Test Recall: {:.4f}\".format(recall))\n",
        "    print(\"Test F1-Score: {:.4f}\".format(f1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiCexAvXTxSR",
        "outputId": "27af6402-c84e-4789-a79f-41406c669893"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Accuracy: 0.9994733330992591\n",
            "Test Precision: 0.9722\n",
            "Test Recall: 0.7143\n",
            "Test F1-Score: 0.8235\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter Selection:\n",
        "\n",
        "I initially set the hidden layer size to 16 based on a balance between model capacity and simplicity. Given that my task is binary classification on the credit card fraud dataset (which has a relatively straightforward decision boundary for distinguishing fraud from non-fraud), a modest number of hidden neurons was sufficient. Preliminary experiments with larger sizes (e.g., 32 or 64 neurons) did not yield significant performance improvements and only worsened overfitting.\n",
        "\n",
        "I started with a learning rate of 0.001. This value was chosen because it typically provides a good balance between convergence speed and stability. I performed a few trials to ensure that the training loss decreased smoothly without oscillations, confirming that this learning rate was appropriate for the problem.\n",
        "\n",
        "A batch size of 128 was selected to balance between the stability of gradient estimates and computational efficiency. Smaller batches can introduce too much noise into the gradient estimates, while very large batches could slow down training and potentially require more memory than is available. The chosen batch size provided stable updates and efficient training given the size of the dataset.\n",
        "\n",
        "I set the number of epochs to 10. More epochs were not necessary to reach convergence.\n",
        "\n",
        "In this project, I did not apply explicit regularization techniques such as dropout or L2 weight decay. The credit card fraud dataset is quite large, which helps mitigate overfitting even with a relatively simple network. In addition, the neural network architecture is a 2-layer network meaning that the model capacity is not excessively high for this task.\n",
        "\n",
        "I chose to use Adam as the optimizer because it automatically adjusts the learning rates for individual parameters based on the first and second moments of the gradients. This adaptive behavior generally leads to faster convergence, especially on problems with noisy or sparse gradients. This was an important consideration in this fraud detection where the minority class is underrepresented."
      ],
      "metadata": {
        "id": "iTe_rObWU19T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4"
      ],
      "metadata": {
        "id": "qZZeQ7DdWIog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "LSQJgZcXWOU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"creditcard.csv\")\n",
        "data.dropna(inplace=True)\n",
        "X = data.drop(\"Class\", axis=1)\n",
        "y = data[\"Class\"]\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train-Dev-Test split\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X_scaled, y, test_size=0.2, stratify=y)\n",
        "X_train, X_dev, y_train, y_dev = train_test_split(X_temp, y_temp, test_size=0.2, stratify=y_temp)\n",
        "\n",
        "# Convert to NumPy arrays.\n",
        "X_train_np = X_train\n",
        "X_dev_np = X_dev\n",
        "X_test_np = X_test\n",
        "y_train_np = y_train.values\n",
        "y_dev_np = y_dev.values\n",
        "y_test_np = y_test.values\n",
        "\n",
        "# Train the Logistic Regression model\n",
        "baseline_model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
        "baseline_model.fit(X_train_np, y_train_np)\n",
        "\n",
        "# Evaluate\n",
        "lr_predictions = baseline_model.predict(X_test_np)\n",
        "\n",
        "# Compute metrics\n",
        "lr_accuracy = accuracy_score(y_test_np, lr_predictions)\n",
        "lr_precision = precision_score(y_test_np, lr_predictions)\n",
        "lr_recall = recall_score(y_test_np, lr_predictions)\n",
        "lr_f1 = f1_score(y_test_np, lr_predictions)\n",
        "\n",
        "print(\"Logistic Regression Baseline Results:\")\n",
        "print(\"Accuracy:  {:.4f}\".format(lr_accuracy))\n",
        "print(\"Precision: {:.4f}\".format(lr_precision))\n",
        "print(\"Recall:    {:.4f}\".format(lr_recall))\n",
        "print(\"F1-Score:  {:.4f}\".format(lr_f1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBzUi5KlWO1q",
        "outputId": "baa7a0dc-da58-4dd1-a133-08cbabf5319e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Baseline Results:\n",
            "Accuracy:  0.9802\n",
            "Precision: 0.0722\n",
            "Recall:    0.8878\n",
            "F1-Score:  0.1335\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural Network has high precision (97.22%), meaning that when it predicts a transaction as fraud, it is almost always correct. However, its recall is lower (71.43%), which indicates that it misses some fraud cases. Logistic regression, on the other hand, achieves a high recall (88.78%), so it catches most of the actual fraud cases, but its precision is extremely low (7.22%). This indicates that it generates a large number of false positives.\n",
        "The F1-score is much higher for the neural network (~82.35%) compared to logistic regression (~13.35%). This suggests that overall, the NN provides a much more reliable performance.\n",
        "\n",
        "The NN is capable of learning non-linear relationships in the data. Fraud detection often involves complex patterns and interactions among features, which a neural network can capture effectively. The NNs hidden layers allow it to build more sophisticated decision boundaries. Logistic regression on the other hand, is a linear model, meaning it can only capture linear relationships. In an imbalanced and complex dataset like credit card fraud, a linear decision boundary may not be sufficient, leading to poor precision despite a high recall. In addition, we are working with a highly imbalanced dataset. The decision threshold can greatly affect performance metrics. Logistic regression in this case, might have been adjusted to favor recall at the cost of precision, resulting in many false positives.\n",
        "\n",
        "In fraud detection, high precision is crucial because each false positive can result in unnecessary investigations or customer inconvenience. The neural networks high precision makes it far more practical for real-world applications.\n",
        "\t\tModel Interpretability vs. Performance:\n",
        "\t\tWhile logistic regression is more interpretable, its performance in this context is significantly worse. In situations where performance is paramount (and additional techniques can be used to interpret NN decisions), the neural networks improved metrics are a compelling advantage."
      ],
      "metadata": {
        "id": "E1rgW_4kXaVw"
      }
    }
  ]
}