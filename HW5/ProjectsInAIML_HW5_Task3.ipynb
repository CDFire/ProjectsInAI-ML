{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbZQ0wH1iPQ8r0IDeGU0Nh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CDFire/ProjectsInAI-ML/blob/main/HW5/ProjectsInAIML_HW5_Task3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 1"
      ],
      "metadata": {
        "id": "mYL-dTjIvjPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math"
      ],
      "metadata": {
        "id": "2AKQAOS9vloG"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V):\n",
        "    d_k = Q.shape[-1]\n",
        "    scores = np.dot(Q, K.T)\n",
        "    scaled_scores = scores / np.sqrt(d_k)\n",
        "    attention_weights = softmax(scaled_scores)\n",
        "    output = np.dot(attention_weights, V)\n",
        "\n",
        "    return output, attention_weights"
      ],
      "metadata": {
        "id": "rqV6GWbavvph"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 2"
      ],
      "metadata": {
        "id": "FIaArW8swHxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rnn_cell(x, h_prev, W_x, W_h, b):\n",
        "    return np.tanh(np.dot(x, W_x) + np.dot(h_prev, W_h) + b)"
      ],
      "metadata": {
        "id": "H_bEZSeCvx5o"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder:\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.W_x = np.random.randn(input_dim, hidden_dim)\n",
        "        self.W_h = np.random.randn(hidden_dim, hidden_dim)\n",
        "        self.b = np.zeros(hidden_dim)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        T = inputs.shape[0]\n",
        "        h = np.zeros(self.hidden_dim)\n",
        "        hidden_states = []\n",
        "        for t in range(T):\n",
        "            h = rnn_cell(inputs[t], h, self.W_x, self.W_h, self.b)\n",
        "            hidden_states.append(h)\n",
        "        hidden_states = np.stack(hidden_states, axis=0)\n",
        "        return hidden_states\n",
        "\n",
        "    def forward_with_attention(self, inputs):\n",
        "        encoder_hidden = self.forward(inputs)\n",
        "        context, attn_weights = scaled_dot_product_attention(encoder_hidden, encoder_hidden, encoder_hidden)\n",
        "        final_context = np.mean(context, axis=0)\n",
        "        return final_context, attn_weights, encoder_hidden\n"
      ],
      "metadata": {
        "id": "iHCv1O31wMkL"
      },
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder:\n",
        "    def __init__(self, emb_dim, hidden_dim, output_dim):\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.emb_dim = emb_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.W_x = np.random.randn(emb_dim, hidden_dim)\n",
        "        self.W_h = np.random.randn(hidden_dim, hidden_dim)\n",
        "        self.b = np.zeros(hidden_dim)\n",
        "\n",
        "        self.W_out = np.random.randn(hidden_dim, output_dim)\n",
        "        self.b_out = np.zeros(output_dim)\n",
        "\n",
        "    def forward(self, initial_state, inputs):\n",
        "        T = inputs.shape[0]\n",
        "        h = initial_state\n",
        "        logits_seq = []\n",
        "        for t in range(T):\n",
        "            h = rnn_cell(inputs[t], h, self.W_x, self.W_h, self.b)\n",
        "            logits = np.dot(h, self.W_out) + self.b_out\n",
        "            logits_seq.append(logits)\n",
        "        return np.stack(logits_seq, axis=0), h"
      ],
      "metadata": {
        "id": "mqLswiJewR8K"
      },
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 3"
      ],
      "metadata": {
        "id": "LdMfXqU5wZ9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_df = pd.read_csv(\"Sentence pairs in English-French - 2025-03-19.tsv\", sep=\"\\t\", header=None,\n",
        "                        names=[\"src_id\", \"english\", \"tgt_id\", \"french\"])\n",
        "data_df = data_df.sample(n=1000).reset_index(drop=True)\n",
        "print(\"Columns in TSV file:\", data_df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmQGvGVvwVnW",
        "outputId": "189a0d6f-b94b-4a88-eed8-a3ee8d42b734"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in TSV file: Index(['src_id', 'english', 'tgt_id', 'french'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = list(zip(data_df['english'], data_df['french']))"
      ],
      "metadata": {
        "id": "SYeSlJdo9fj3"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_index = int(0.9 * len(data))\n",
        "train_data = data[:split_index]\n",
        "test_data = data[split_index:]"
      ],
      "metadata": {
        "id": "c6p9INYlxRK0"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(sentences):\n",
        "    vocab = {\"<pad>\":0, \"<sos>\":1, \"<eos>\":2, \"<unk>\":3}\n",
        "    idx = 4\n",
        "    for sent in sentences:\n",
        "        for word in sent.split():\n",
        "            if word not in vocab:\n",
        "                vocab[word] = idx\n",
        "                idx += 1\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "fUjp34gAxTGb"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentences = [pair[0] for pair in train_data]\n",
        "tgt_sentences = [pair[1] for pair in train_data]\n",
        "\n",
        "src_vocab = build_vocab(src_sentences)\n",
        "tgt_vocab = build_vocab(tgt_sentences)\n",
        "\n",
        "inv_tgt_vocab = {i: w for w, i in tgt_vocab.items()}"
      ],
      "metadata": {
        "id": "LdFF-aGFxUfI"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb_dim = 8\n",
        "hidden_dim = 16\n",
        "lr = 0.001\n",
        "num_epochs = 3000\n",
        "\n",
        "src_vocab_size = len(src_vocab)\n",
        "tgt_vocab_size = len(tgt_vocab)\n",
        "\n",
        "E_src = np.random.randn(src_vocab_size, emb_dim)\n",
        "E_tgt = np.random.randn(tgt_vocab_size, emb_dim)"
      ],
      "metadata": {
        "id": "knecsnJixXK9"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(sentence, vocab, add_special_tokens=False):\n",
        "    tokens = sentence.split()\n",
        "    if add_special_tokens:\n",
        "        tokens = [\"<sos>\"] + tokens + [\"<eos>\"]\n",
        "    return [vocab.get(w, vocab[\"<unk>\"]) for w in tokens]\n",
        "\n",
        "def detokenize(token_ids, inv_vocab):\n",
        "    words = []\n",
        "    for tid in token_ids:\n",
        "        word = inv_vocab.get(tid, \"<unk>\")\n",
        "        if word in [\"<sos>\", \"<eos>\", \"<pad>\"]:\n",
        "            continue\n",
        "        words.append(word)\n",
        "    return \" \".join(words)"
      ],
      "metadata": {
        "id": "Ix5eY_aYxbow"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(logits, target_idx):\n",
        "    probs = softmax(logits)\n",
        "    loss = -np.log(probs[target_idx] + 1e-9)\n",
        "    return loss, probs"
      ],
      "metadata": {
        "id": "tHUiBzHIxdZD"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_output_layer(decoder, h, logits, target_idx, lr):\n",
        "    probs = softmax(logits)\n",
        "    grad_logits = probs.copy()\n",
        "    grad_logits[target_idx] -= 1\n",
        "    grad_W_out = np.outer(h, grad_logits)\n",
        "    grad_b_out = grad_logits\n",
        "    decoder.W_out -= lr * grad_W_out\n",
        "    decoder.b_out -= lr * grad_b_out"
      ],
      "metadata": {
        "id": "oIhPSwePxiUd"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(emb_dim, hidden_dim)\n",
        "decoder = Decoder(emb_dim, hidden_dim, tgt_vocab_size)"
      ],
      "metadata": {
        "id": "ReKlZaRSxoIf"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting training...\")\n",
        "\n",
        "best_loss = float('inf')\n",
        "patience = 10\n",
        "lr_decay = 0.5\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0.0\n",
        "    for src_sent, tgt_sent in train_data:\n",
        "        src_indices = tokenize(src_sent, src_vocab, add_special_tokens=False)\n",
        "        tgt_indices = tokenize(tgt_sent, tgt_vocab, add_special_tokens=True)\n",
        "\n",
        "        src_embeds = np.array([E_src[idx] for idx in src_indices])\n",
        "        tgt_embeds = np.array([E_tgt[idx] for idx in tgt_indices])\n",
        "\n",
        "        context, attn_weights, encoder_hidden = encoder.forward_with_attention(src_embeds)\n",
        "\n",
        "        logits_seq, last_h = decoder.forward(context, tgt_embeds[:-1])\n",
        "\n",
        "        example_loss = 0.0\n",
        "        T_dec = logits_seq.shape[0]\n",
        "        for t in range(T_dec):\n",
        "            loss, _ = cross_entropy_loss(logits_seq[t], tgt_indices[t+1])\n",
        "            example_loss += loss\n",
        "            update_output_layer(decoder, last_h, logits_seq[t], tgt_indices[t+1], lr)\n",
        "        total_loss += example_loss\n",
        "\n",
        "    if total_loss < best_loss:\n",
        "        best_loss = total_loss\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            lr *= lr_decay\n",
        "            if(lr < 1e-6):\n",
        "                break\n",
        "            print(f\"Epoch {epoch+1}: Loss did not improve for {patience} epochs. Reducing learning rate to {lr:.6f}.\")\n",
        "            patience_counter = 0\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szEpW-XtxpmJ",
        "outputId": "d7746642-3f2b-4c0e-a906-033046cea028"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 10/3000, Loss: 90469.1265\n",
            "Epoch 20/3000, Loss: 86848.3948\n",
            "Epoch 30/3000, Loss: 84440.5693\n",
            "Epoch 40/3000, Loss: 83000.1977\n",
            "Epoch 50/3000, Loss: 82237.7232\n",
            "Epoch 60/3000, Loss: 81844.4776\n",
            "Epoch 70/3000, Loss: 81707.9366\n",
            "Epoch 80/3000, Loss: 81775.5020\n",
            "Epoch 81: Loss did not improve for 10 epochs. Reducing learning rate to 0.000500.\n",
            "Epoch 90/3000, Loss: 81888.5729\n",
            "Epoch 91: Loss did not improve for 10 epochs. Reducing learning rate to 0.000250.\n",
            "Epoch 100/3000, Loss: 81963.5946\n",
            "Epoch 101: Loss did not improve for 10 epochs. Reducing learning rate to 0.000125.\n",
            "Epoch 110/3000, Loss: 82005.4445\n",
            "Epoch 111: Loss did not improve for 10 epochs. Reducing learning rate to 0.000063.\n",
            "Epoch 120/3000, Loss: 82027.4205\n",
            "Epoch 121: Loss did not improve for 10 epochs. Reducing learning rate to 0.000031.\n",
            "Epoch 130/3000, Loss: 82038.6672\n",
            "Epoch 131: Loss did not improve for 10 epochs. Reducing learning rate to 0.000016.\n",
            "Epoch 140/3000, Loss: 82044.3547\n",
            "Epoch 141: Loss did not improve for 10 epochs. Reducing learning rate to 0.000008.\n",
            "Epoch 150/3000, Loss: 82047.2144\n",
            "Epoch 151: Loss did not improve for 10 epochs. Reducing learning rate to 0.000004.\n",
            "Epoch 160/3000, Loss: 82048.6483\n",
            "Epoch 161: Loss did not improve for 10 epochs. Reducing learning rate to 0.000002.\n",
            "Epoch 170/3000, Loss: 82049.3662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_decode(encoder, decoder, src_sentence, max_len=10):\n",
        "    src_indices = tokenize(src_sentence, src_vocab, add_special_tokens=False)\n",
        "    src_embeds = np.array([E_src[idx] for idx in src_indices])\n",
        "    context, _, _ = encoder.forward_with_attention(src_embeds)\n",
        "    current_token = tgt_vocab[\"<sos>\"]\n",
        "    decoded_tokens = []\n",
        "    h = context\n",
        "    for _ in range(max_len):\n",
        "        token_embed = E_tgt[current_token]\n",
        "        h = rnn_cell(token_embed, h, decoder.W_x, decoder.W_h, decoder.b)\n",
        "        logits = np.dot(h, decoder.W_out) + decoder.b_out\n",
        "        probs = softmax(logits)\n",
        "        current_token = np.argmax(probs)\n",
        "        if current_token == tgt_vocab[\"<eos>\"]:\n",
        "            break\n",
        "        decoded_tokens.append(current_token)\n",
        "    return detokenize(decoded_tokens, inv_tgt_vocab)\n"
      ],
      "metadata": {
        "id": "FKJvtZU2x6ym"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_bleu(reference, candidate):\n",
        "    ref_tokens = reference.split()\n",
        "    cand_tokens = candidate.split()\n",
        "\n",
        "    if len(cand_tokens) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    ref_counts = {}\n",
        "    for token in ref_tokens:\n",
        "        ref_counts[token] = ref_counts.get(token, 0) + 1\n",
        "    cand_counts = {}\n",
        "    for token in cand_tokens:\n",
        "        cand_counts[token] = cand_counts.get(token, 0) + 1\n",
        "\n",
        "    overlap = 0\n",
        "    for token in cand_counts:\n",
        "        overlap += min(cand_counts[token], ref_counts.get(token, 0))\n",
        "    precision = overlap / len(cand_tokens)\n",
        "\n",
        "    bp = 1.0 if len(cand_tokens) >= len(ref_tokens) else math.exp(1 - len(ref_tokens) / len(cand_tokens))\n",
        "    bleu = bp * precision\n",
        "    return bleu"
      ],
      "metadata": {
        "id": "hNOvmUDKyDKf"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_scores = []\n",
        "print(\"\\n--- Test Set Evaluation ---\")\n",
        "for src_sent, ref_sent in test_data:\n",
        "    pred_sent = greedy_decode(encoder, decoder, src_sent)\n",
        "    bleu = compute_bleu(ref_sent, pred_sent)\n",
        "    bleu_scores.append(bleu)\n",
        "    print(f\"Source: {src_sent}\")\n",
        "    print(f\"Reference: {ref_sent}\")\n",
        "    print(f\"Prediction: {pred_sent}\")\n",
        "    print(f\"BLEU-1: {bleu:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6QXwq4LyE3I",
        "outputId": "9bb8b2ec-af3f-44bc-d68e-e79aaadd336c"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Test Set Evaluation ---\n",
            "Source: Do you want to play a game?\n",
            "Reference: Voulez-vous jouer à un jeu ?\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: I think we've met before.\n",
            "Reference: Je pense que nous nous sommes déjà rencontrés.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: Do you remember your grandfather?\n",
            "Reference: Vous souvenez-vous de votre grand-père ?\n",
            "Prediction: sexuellement à composantes à ne à mouches de la n'aurais\n",
            "BLEU-1: 0.1000\n",
            "\n",
            "Source: She wrapped herself in a wool blanket.\n",
            "Reference: Elle s'est enroulée dans une couverture en laine.\n",
            "Prediction: que Tom que Nous pas regardait\n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: It's a custom to celebrate Christmas.\n",
            "Reference: C'est une coutume de célébrer Noël.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: The price doesn't matter.\n",
            "Reference: Le prix n'est pas important.\n",
            "Prediction: que Tom que a pas de\n",
            "BLEU-1: 0.1667\n",
            "\n",
            "Source: Your mother is worried sick about you.\n",
            "Reference: Ta mère est morte d'inquiétude à ton sujet.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: Would you pardon me one moment, please?\n",
            "Reference: Voudriez-vous m'excuser un moment ?\n",
            "Prediction: toit\n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: You're behaving like a spoiled brat.\n",
            "Reference: Tu te comportes comme un enfant gâté.\n",
            "Prediction: pense de Je\n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: He made me love jazz.\n",
            "Reference: Il m'a fait aimer le jazz.\n",
            "Prediction: pense à Ayant à Ayant Je a marche ?\n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: Are you my real mother?\n",
            "Reference: Êtes-vous ma vrai mère ?\n",
            "Prediction: haut !\n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: She's a keeper.\n",
            "Reference: On garde celui-ci.\n",
            "Prediction: pas à blanc Je ne à ne ne que à\n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: Yanni didn't really understand why.\n",
            "Reference: Yanni ne comprenait pas vraiment pourquoi.\n",
            "Prediction: anglais. ! Je se Mary Bruce ! ne est embrassé\n",
            "BLEU-1: 0.1000\n",
            "\n",
            "Source: School starts next Monday.\n",
            "Reference: L'école commence lundi prochain.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: You make me happy.\n",
            "Reference: Vous me rendez heureux.\n",
            "Prediction: pour ! jour Je ne à cheval. à ne à\n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: He decided he didn't love me anymore.\n",
            "Reference: Il a décidé qu'il ne m'aimait plus.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: Tom will buy that painting. \"How can you be sure?\"\n",
            "Reference: « Tom achètera ce tableau. » « Comment pouvez-vous en être sûr ? »\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: Will you be ready soon?\n",
            "Reference: Es-tu bientôt prêt ?\n",
            "Prediction: de sais-tu valises de suis de MDR. de le dans\n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: I think that Tom injured himself.\n",
            "Reference: Je pense que Tom s'est blessé.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: I agree with you on this issue.\n",
            "Reference: Je suis d'accord avec vous sur ce point.\n",
            "Prediction: l'ai bon. boire\n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: Are you sure this is a good idea?\n",
            "Reference: Es-tu sûre que ce soit une bonne idée ?\n",
            "Prediction: anglais. ! Je se utilité. Tom\n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: I know Tom isn't a Canadian.\n",
            "Reference: Je sais que Tom n'est pas Canadien.\n",
            "Prediction: de MDR. de le dans le dans ait a\n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: They came even though it was raining.\n",
            "Reference: Ils vinrent malgré la pluie.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: He ran faster than his brother did.\n",
            "Reference: Il courait plus vite que son frère.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: What is Mr. Johnson's first name?\n",
            "Reference: Comment M. Johnson se prénomme-t-il?\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: There's someone behind you.\n",
            "Reference: Il y a quelqu'un derrière vous.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: Overfishing is a major problem.\n",
            "Reference: La surpêche est un problème majeur.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: Why am I shooting these guys, tell me, what have they done?\n",
            "Reference: Pourquoi est-ce que je tire sur ces types, dis-moi, qu'ont-ils fait ?\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: I haven't done the dishes yet.\n",
            "Reference: Je n'ai pas encore fait la vaisselle.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: He will walk in the park this afternoon.\n",
            "Reference: Il marchera dans le parc cet après-midi.\n",
            "Prediction: tamazight rendre des de ! dans le Il le dans\n",
            "BLEU-1: 0.3000\n",
            "\n",
            "Source: Hurry up, or we'll miss the train.\n",
            "Reference: Plus vite, ou nous allons rater le train.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: The avenue is full of traffic lights.\n",
            "Reference: L'avenue est pleine de feux de signalisation.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: If I was rich I'd buy a house by the sea.\n",
            "Reference: Si j'avais de l'argent plein les poches, je m'achèterais une maison au bord de la mer.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: He is a moocher and never pays for anything.\n",
            "Reference: C'est un profiteur qui ne paie jamais rien.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: Let me check my Rolodex.\n",
            "Reference: Permettez-moi de vérifier mon Rolodex.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: Here's the doctor.\n",
            "Reference: Voici le médecin.\n",
            "Prediction: retour\n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: That's all true.\n",
            "Reference: Tout cela est vrai.\n",
            "Prediction: toit\n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: It's important to read books.\n",
            "Reference: Il est important de lire des livres.\n",
            "Prediction: Ne suis de le de le jury le jettes le\n",
            "BLEU-1: 0.1000\n",
            "\n",
            "Source: She pretended to be ill.\n",
            "Reference: Elle fit comme si elle était malade.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: Don't let the cat out.\n",
            "Reference: Ne laisse pas sortir le chat.\n",
            "Prediction: pour pomme. Il pomme. aide des des de la endroit.\n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: Come over!\n",
            "Reference: Venez !\n",
            "Prediction: recherche public. une\n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: The stakes are high.\n",
            "Reference: Les enjeux sont élevés.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: I would like to charter a yacht.\n",
            "Reference: J'aimerais bien affréter un yacht.\n",
            "Prediction: Ce de la\n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: They hate me 'cause they ain't me!\n",
            "Reference: Ils me haïssent car ils ne sont point moi !\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: Don't let Tom do that here!\n",
            "Reference: Ne laisse pas Tom faire cela ici !\n",
            "Prediction: toit\n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: We were diving.\n",
            "Reference: Nous plongions.\n",
            "Prediction: que serait pouvais\n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: You risk losing my trust.\n",
            "Reference: Tu risques de perdre ma confiance.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: I study French in addition to English.\n",
            "Reference: J'étudie le français en plus de l'anglais.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: No, I don't have it with me.\n",
            "Reference: Non, je ne l'ai pas sur moi.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: We're right.\n",
            "Reference: Nous avons raison.\n",
            "Prediction: n'a l'ai bon. choses dialecte pas un. Je ne Je\n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: There are various ways we could handle this problem.\n",
            "Reference: Il existe plusieurs façons de traiter ce problème.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: Eat plenty of vegetables.\n",
            "Reference: Mangez beaucoup de légumes.\n",
            "Prediction: toit\n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: I'm not really surprised.\n",
            "Reference: Je ne suis pas vraiment surpris.\n",
            "Prediction: de\n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: That bridge isn't strong enough to support so much weight.\n",
            "Reference: Ce pont n'est pas assez solide pour supporter un tel poids.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: What is Russia's capital?\n",
            "Reference: Quelle est la capitale de la Russie ?\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: I don't think it'll rain this afternoon.\n",
            "Reference: Je ne pense pas qu'il pleuvra cet après-midi.\n",
            "Prediction: médecin ta Je se utilité. chouette.\n",
            "BLEU-1: 0.1194\n",
            "\n",
            "Source: Ziri ate gooseberries.\n",
            "Reference: Ziri a mangé des groseilles.\n",
            "Prediction: le suis le jury le Il de lac. toit\n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: I am the best.\n",
            "Reference: Je suis le meilleur.\n",
            "Prediction: mieux est mieux\n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: He thinks that he is a great poet.\n",
            "Reference: Il pense être un grand poète.\n",
            "Prediction: Ne ! souvent.\n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: You're a real genius.\n",
            "Reference: Tu es vraiment un génie.\n",
            "Prediction: réunion de suis\n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: Do you live in Lebanon?\n",
            "Reference: Vis-tu au Liban ?\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: I am very glad to know you.\n",
            "Reference: Je suis très heureuse de te connaître.\n",
            "Prediction: de MDR. de le dans un moi. Je que déferla\n",
            "BLEU-1: 0.2000\n",
            "\n",
            "Source: My train left at 7 and arrived in New York at 10.\n",
            "Reference: Mon train est parti à sept heures et il est arrivé à dix heures à New York.\n",
            "Prediction: que Tom que\n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: I'm feeling good.\n",
            "Reference: Je me sens bien.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: He strenuously denied the claims made against him.\n",
            "Reference: Il a nié vigoureusement les allégations faites contre lui.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: Are you going there on business?\n",
            "Reference: Vous y rendrez-vous pour le travail ?\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: Is that not clear?\n",
            "Reference: N'est-ce pas clair ?\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: This is dangerous territory.\n",
            "Reference: C'est un territoire dangereux.\n",
            "Prediction: d'élèves Tom que maison a\n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: I just saw a huge owl.\n",
            "Reference: Je viens de voir une énorme chouette.\n",
            "Prediction: haut est\n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: I just know that it's not right.\n",
            "Reference: Je sais juste que ce n'est pas correct.\n",
            "Prediction: de sais-tu valises de propriétaire ? ? ? d'emprisonnement. la\n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: I have a call.\n",
            "Reference: J'ai un appel.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: Nothing seemed out of the ordinary.\n",
            "Reference: Rien ne semblait sortir de l'ordinaire.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: Blindfold Tom.\n",
            "Reference: Bande les yeux de Tom.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: I would like to ask a favor of you.\n",
            "Reference: Je voudrais te demander une faveur.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: What happens if I press this button?\n",
            "Reference: Qu'arrive-t-il si j'appuie sur ce bouton ?\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: Everyone was horrified.\n",
            "Reference: Tout le monde fut horrifié.\n",
            "Prediction: pas de la\n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: Please clean up this room before everyone arrives.\n",
            "Reference: S'il vous plaît, nettoyez cette pièce avant que tout le monde n'arrive.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: Can you pass me the thingy?\n",
            "Reference: Tu peux me passer le machin ?\n",
            "Prediction: Ne ! souvent.\n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: That's a beautiful story.\n",
            "Reference: C'est une belle histoire.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: What you give to the wicked, you always regret.\n",
            "Reference: Ce qu'on donne aux méchants, toujours on le regrette.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: Did you enjoy the party yesterday?\n",
            "Reference: Vous êtes-vous amusées hier à la fête ?\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: I'm all thumbs.\n",
            "Reference: Je suis maladroit.\n",
            "Prediction: serait est est est est dépendance\n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: How did you turn off the alarm?\n",
            "Reference: Comment avez-vous arrêté l'alarme ?\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: He's a man of many talents.\n",
            "Reference: C'est un homme aux multiples talents.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: Prices are reasonable.\n",
            "Reference: Les prix sont raisonnables.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: Granting that it is so, what follows?\n",
            "Reference: En admettant qu'il en soit ainsi, que s'ensuit-il ?\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: I know that I'm right.\n",
            "Reference: Je sais que j'ai raison.\n",
            "Prediction: pense à ne à mouches de maison Je que a\n",
            "BLEU-1: 0.2000\n",
            "\n",
            "Source: Let him in.\n",
            "Reference: Faites-le entrer.\n",
            "Prediction: pense de Je\n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: There's something rotten in the state of Denmark.\n",
            "Reference: Il y a quelque chose de pourri dans l'état du Danemark.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: I can't make any promises.\n",
            "Reference: Je ne peux rien promettre.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: Thanks.\n",
            "Reference: Je te remercie.\n",
            "Prediction: pas m'oublie comment\n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: Does he play the piano?\n",
            "Reference: Joue-t-il du piano ?\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: Where's your suitcase?\n",
            "Reference: Où est ta valise ?\n",
            "Prediction: de est souffre nom\n",
            "BLEU-1: 0.1947\n",
            "\n",
            "Source: You should choose a job in relation to your talents and interests.\n",
            "Reference: Vous devriez choisir un emploi en rapport avec vos talents et vos intérêts.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: The typhoon became weaker and changed into a storm.\n",
            "Reference: Le typhon s'est affaibli et s'est transformé en tempête.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: I could not keep the tears from my eyes.\n",
            "Reference: Je n'ai pas pu m'empêcher de pleurer.\n",
            "Prediction: tamazight sais-tu\n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: Can you accompany me there, please?\n",
            "Reference: Pourriez-vous m'accompagner là-bas s'il vous plaît ?\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: They both laughed.\n",
            "Reference: Toutes deux ont ri.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: It's not the same person.\n",
            "Reference: Ce n'est pas la même personne.\n",
            "Prediction: \n",
            "BLEU-1: 0.0000\n",
            "\n",
            "Source: You can't get a tree to grow on bad soil.\n",
            "Reference: Tu auras du mal à faire pousser un arbre sur un mauvais sol.\n",
            "Prediction: Ne ! souvent.\n",
            "BLEU-1: 0.0000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_bleu = np.mean(bleu_scores)\n",
        "print(f\"Average BLEU-1 Score on Test Set: {avg_bleu:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHiZE0NAyGWJ",
        "outputId": "98779745-2b3a-415e-dc6d-706404ba2cfb"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average BLEU-1 Score on Test Set: 0.0148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 4"
      ],
      "metadata": {
        "id": "naCeXCEV1Qg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "9yesXy_KHZGl"
      },
      "execution_count": 278,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(sentences):\n",
        "    vocab = {\"<pad>\":0, \"<sos>\":1, \"<eos>\":2, \"<unk>\":3}\n",
        "    idx = 4\n",
        "    for sent in sentences:\n",
        "        for word in sent.split():\n",
        "            if word not in vocab:\n",
        "                vocab[word] = idx\n",
        "                idx += 1\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "WswWyagHIwaO"
      },
      "execution_count": 279,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df = pd.read_csv(\"Sentence pairs in English-French - 2025-03-19.tsv\", sep=\"\\t\", header=None,\n",
        "                        names=[\"src_id\", \"english\", \"tgt_id\", \"french\"])\n",
        "data_df = data_df.sample(n=10000).reset_index(drop=True)\n",
        "data = list(zip(data_df['english'], data_df['french']))\n",
        "split_index = int(0.9 * len(data))\n",
        "train_data = data[:split_index]\n",
        "test_data = data[split_index:]"
      ],
      "metadata": {
        "id": "8ZW1P-G9I7SY"
      },
      "execution_count": 280,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentences = [pair[0] for pair in train_data]\n",
        "tgt_sentences = [pair[1] for pair in train_data]\n",
        "\n",
        "src_vocab = build_vocab(src_sentences)\n",
        "tgt_vocab = build_vocab(tgt_sentences)\n",
        "\n",
        "inv_tgt_vocab = {i: w for w, i in tgt_vocab.items()}"
      ],
      "metadata": {
        "id": "2FdAayZiIzAz"
      },
      "execution_count": 281,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(sentence, vocab):\n",
        "    return [vocab.get(tok, vocab[\"<unk>\"]) for tok in sentence.strip().split()]\n",
        "\n",
        "src_data = [tokenize(sent, src_vocab) for sent in src_sentences]\n",
        "tgt_data = [tokenize(sent, tgt_vocab) for sent in tgt_sentences]\n",
        "\n",
        "train_size = int(len(src_data)*0.8)\n",
        "src_train, src_val = src_data[:train_size], src_data[train_size:]\n",
        "tgt_train, tgt_val = tgt_data[:train_size], tgt_data[train_size:]"
      ],
      "metadata": {
        "id": "3e9KHTGHJK-Y"
      },
      "execution_count": 282,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        x = x + self.pe[:, :seq_len, :]\n",
        "        return x"
      ],
      "metadata": {
        "id": "bqd7ACrpJO4H"
      },
      "execution_count": 283,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
        "    d_k = Q.size(-1)\n",
        "    scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)\n",
        "\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "    attn = torch.softmax(scores, dim=-1)\n",
        "    output = torch.matmul(attn, V)\n",
        "    return output, attn"
      ],
      "metadata": {
        "id": "z5AdFM5MJXcL"
      },
      "execution_count": 284,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model=64, n_heads=2):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        batch_size = Q.size(0)\n",
        "\n",
        "        Q = self.W_q(Q)\n",
        "        K = self.W_k(K)\n",
        "        V = self.W_v(V)\n",
        "\n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1)\n",
        "\n",
        "        attn_output, attn = scaled_dot_product_attention(Q, K, V, mask=mask)\n",
        "\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "        attn_output = attn_output.view(batch_size, -1, self.d_model)\n",
        "\n",
        "        output = self.W_o(attn_output)\n",
        "\n",
        "        return output, attn"
      ],
      "metadata": {
        "id": "4dhAbq4cJc6P"
      },
      "execution_count": 285,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model=64, dim_ff=128):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, dim_ff)\n",
        "        self.fc2 = nn.Linear(dim_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))"
      ],
      "metadata": {
        "id": "7DZCVqr3JoX9"
      },
      "execution_count": 286,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model=64, n_heads=2, dim_ff=128):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_heads)\n",
        "        self.ff = PositionwiseFeedForward(d_model, dim_ff)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, src_mask=None):\n",
        "        # x shape: (batch_size, src_len, d_model)\n",
        "\n",
        "        # 1) Self-attention\n",
        "        attn_output, _ = self.self_attn(x, x, x, mask=src_mask)\n",
        "        x = x + attn_output  # Residual\n",
        "        x = self.norm1(x)    # Layer norm\n",
        "\n",
        "        # 2) Feed-forward\n",
        "        ff_output = self.ff(x)\n",
        "        x = x + ff_output\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "jygvkdJ3Jrxb"
      },
      "execution_count": 287,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model=64, n_heads=2, dim_ff=128):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_heads)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, n_heads)\n",
        "        self.ff = PositionwiseFeedForward(d_model, dim_ff)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, enc_out, tgt_mask=None, src_mask=None):\n",
        "        # 1) Masked self-attention in decoder\n",
        "        _x, _ = self.self_attn(x, x, x, mask=tgt_mask)\n",
        "        x = x + _x\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        # 2) Cross-attention: Q=decoder states, K=encoder outputs, V=encoder outputs\n",
        "        _x, attn = self.cross_attn(x, enc_out, enc_out, mask=src_mask)  # typically no mask for cross-attn unless padding\n",
        "        x = x + _x\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        # 3) Position-wise feed-forward\n",
        "        _x = self.ff(x)\n",
        "        x = x + _x\n",
        "        x = self.norm3(x)\n",
        "\n",
        "        return x, attn"
      ],
      "metadata": {
        "id": "fsrU75GGJtWF"
      },
      "execution_count": 288,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, src_vocab_size, d_model=64, n_heads=2, dim_ff=128, num_layers=2, max_len=100):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, n_heads, dim_ff) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, src, src_mask=None):\n",
        "        # src shape: (batch_size, src_len)\n",
        "        x = self.embedding(src) * np.sqrt(self.d_model)\n",
        "        x = self.pos_encoding(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, src_mask)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, tgt_vocab_size, d_model=64, n_heads=2, dim_ff=128, num_layers=2, max_len=100):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, n_heads, dim_ff) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, tgt, enc_out, tgt_mask=None, src_mask=None):\n",
        "        # tgt shape: (batch_size, tgt_len)\n",
        "        x = self.embedding(tgt) * np.sqrt(self.d_model)\n",
        "        x = self.pos_encoding(x)\n",
        "\n",
        "        attn_weights = None\n",
        "        for layer in self.layers:\n",
        "            x, attn_weights = layer(x, enc_out, tgt_mask, src_mask)\n",
        "        return x, attn_weights"
      ],
      "metadata": {
        "id": "XYt7RAuAJu8L"
      },
      "execution_count": 289,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        src_vocab_size,\n",
        "        tgt_vocab_size,\n",
        "        d_model=64,\n",
        "        n_heads=2,\n",
        "        dim_ff=128,\n",
        "        num_enc_layers=2,\n",
        "        num_dec_layers=2,\n",
        "        max_len=100\n",
        "    ):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            src_vocab_size, d_model, n_heads, dim_ff, num_enc_layers, max_len\n",
        "        )\n",
        "        self.decoder = Decoder(\n",
        "            tgt_vocab_size, d_model, n_heads, dim_ff, num_dec_layers, max_len\n",
        "        )\n",
        "        self.output_projection = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "    def generate_src_mask(self, src):\n",
        "        pad_token = 0\n",
        "        mask = (src != pad_token).unsqueeze(1)\n",
        "        return mask\n",
        "\n",
        "    def generate_tgt_mask(self, tgt):\n",
        "        batch_size, tgt_len = tgt.size()\n",
        "        pad_token = 0\n",
        "\n",
        "        padding_mask = (tgt != pad_token).unsqueeze(1)\n",
        "\n",
        "        subsequent_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=tgt.device)).bool()\n",
        "        subsequent_mask = subsequent_mask.unsqueeze(0)\n",
        "\n",
        "        combined_mask = padding_mask & subsequent_mask\n",
        "        return combined_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask = self.generate_src_mask(src)\n",
        "        tgt_mask = self.generate_tgt_mask(tgt)\n",
        "\n",
        "        enc_out = self.encoder(src, src_mask=src_mask)\n",
        "        dec_out, attn_weights = self.decoder(tgt, enc_out, tgt_mask=tgt_mask, src_mask=src_mask)\n",
        "\n",
        "        logits = self.output_projection(dec_out)\n",
        "        return logits, attn_weights\n"
      ],
      "metadata": {
        "id": "je10po4OJwux"
      },
      "execution_count": 290,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "SRC_VOCAB_SIZE = len(src_vocab)\n",
        "TGT_VOCAB_SIZE = len(tgt_vocab)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 30\n",
        "MAX_LEN = 50\n",
        "\n",
        "model = Transformer(\n",
        "    src_vocab_size=SRC_VOCAB_SIZE,\n",
        "    tgt_vocab_size=TGT_VOCAB_SIZE,\n",
        "    d_model=64,\n",
        "    n_heads=2,\n",
        "    dim_ff=128,\n",
        "    num_enc_layers=2,\n",
        "    num_dec_layers=2,\n",
        "    max_len=MAX_LEN\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "def pad_batch(sentences, pad_id=0, max_len=MAX_LEN):\n",
        "    batch_size = len(sentences)\n",
        "    padded = torch.full((batch_size, max_len), pad_id, dtype=torch.long)\n",
        "    for i, seq in enumerate(sentences):\n",
        "        length = min(len(seq), max_len)\n",
        "        padded[i, :length] = torch.tensor(seq[:length], dtype=torch.long)\n",
        "    return padded\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for i in range(0, len(src_train), BATCH_SIZE):\n",
        "        src_batch = src_train[i:i+BATCH_SIZE]\n",
        "        tgt_batch = tgt_train[i:i+BATCH_SIZE]\n",
        "\n",
        "        if len(src_batch) < BATCH_SIZE:\n",
        "            break\n",
        "\n",
        "        tgt_in = [[1] + seq for seq in tgt_batch]\n",
        "        tgt_out = [seq + [2] for seq in tgt_batch]\n",
        "\n",
        "        src_tensor = pad_batch(src_batch).to(device)\n",
        "        tgt_in_tensor = pad_batch(tgt_in).to(device)\n",
        "        tgt_out_tensor = pad_batch(tgt_out).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits, _ = model(src_tensor, tgt_in_tensor)\n",
        "\n",
        "        logits = logits.view(-1, TGT_VOCAB_SIZE)\n",
        "        tgt_out_tensor = tgt_out_tensor.view(-1)\n",
        "\n",
        "        loss = criterion(logits, tgt_out_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / (len(src_train)//BATCH_SIZE)\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUXjH9HcJ65l",
        "outputId": "b27a8fca-ccea-4107-f855-f00c5f4672e8"
      },
      "execution_count": 291,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30, Loss: 6.6931\n",
            "Epoch 2/30, Loss: 5.3778\n",
            "Epoch 3/30, Loss: 4.7565\n",
            "Epoch 4/30, Loss: 4.2434\n",
            "Epoch 5/30, Loss: 3.7912\n",
            "Epoch 6/30, Loss: 3.3700\n",
            "Epoch 7/30, Loss: 2.9655\n",
            "Epoch 8/30, Loss: 2.5912\n",
            "Epoch 9/30, Loss: 2.2466\n",
            "Epoch 10/30, Loss: 1.9417\n",
            "Epoch 11/30, Loss: 1.6806\n",
            "Epoch 12/30, Loss: 1.4654\n",
            "Epoch 13/30, Loss: 1.2867\n",
            "Epoch 14/30, Loss: 1.1404\n",
            "Epoch 15/30, Loss: 1.0112\n",
            "Epoch 16/30, Loss: 0.8983\n",
            "Epoch 17/30, Loss: 0.8050\n",
            "Epoch 18/30, Loss: 0.7114\n",
            "Epoch 19/30, Loss: 0.6294\n",
            "Epoch 20/30, Loss: 0.5677\n",
            "Epoch 21/30, Loss: 0.5036\n",
            "Epoch 22/30, Loss: 0.4639\n",
            "Epoch 23/30, Loss: 0.4178\n",
            "Epoch 24/30, Loss: 0.3665\n",
            "Epoch 25/30, Loss: 0.3357\n",
            "Epoch 26/30, Loss: 0.3140\n",
            "Epoch 27/30, Loss: 0.2892\n",
            "Epoch 28/30, Loss: 0.2662\n",
            "Epoch 29/30, Loss: 0.2557\n",
            "Epoch 30/30, Loss: 0.2347\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
      ],
      "metadata": {
        "id": "KWXcSDB3KK5n"
      },
      "execution_count": 293,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_decode(model, src, max_len=50, start_token=1, end_token=2):\n",
        "    model.eval()\n",
        "\n",
        "    batch_size = src.size(0)\n",
        "    tgt_generated = torch.full((batch_size, 1), start_token, dtype=torch.long, device=src.device)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        logits, _ = model(src, tgt_generated)\n",
        "        next_token_logits = logits[:, -1, :]\n",
        "        next_tokens = next_token_logits.argmax(dim=-1, keepdim=True)\n",
        "\n",
        "        tgt_generated = torch.cat([tgt_generated, next_tokens], dim=1)\n",
        "\n",
        "    return tgt_generated.tolist()\n",
        "\n",
        "    if count == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return total_bleu / count"
      ],
      "metadata": {
        "id": "8ANZtQvHayky"
      },
      "execution_count": 295,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_bleu(model, src_val, tgt_val, batch_size=32, max_len=50):\n",
        "    model.eval()\n",
        "    smoothie = SmoothingFunction().method1\n",
        "\n",
        "    total_bleu = 0.0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(src_val), batch_size):\n",
        "            src_batch = src_val[i : i + batch_size]\n",
        "            ref_batch = tgt_val[i : i + batch_size]\n",
        "\n",
        "            if len(src_batch) < 1:\n",
        "                break\n",
        "\n",
        "            src_tensor = pad_batch(src_batch).to(device)\n",
        "\n",
        "            generated_batch = greedy_decode(model, src_tensor, max_len=max_len)\n",
        "\n",
        "            for gen_ids, ref_ids in zip(generated_batch, ref_batch):\n",
        "\n",
        "                if 2 in gen_ids:\n",
        "                    eos_idx = gen_ids.index(2)\n",
        "                    gen_ids = gen_ids[1:eos_idx]\n",
        "                else:\n",
        "                    gen_ids = gen_ids[1:]\n",
        "\n",
        "                if 2 in ref_ids:\n",
        "                    ref_eos_idx = ref_ids.index(2)\n",
        "                    ref_ids = ref_ids[:ref_eos_idx]\n",
        "                if 1 in ref_ids:\n",
        "                    ref_ids = ref_ids[1:]\n",
        "\n",
        "                reference = [ref_ids]\n",
        "                hypothesis = gen_ids\n",
        "\n",
        "                bleu_score = sentence_bleu(reference, hypothesis, smoothing_function=smoothie)\n",
        "                total_bleu += bleu_score\n",
        "                count += 1\n",
        "\n",
        "    return total_bleu / count if count > 0 else 0.0"
      ],
      "metadata": {
        "id": "e_-BND0eb8-k"
      },
      "execution_count": 298,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_bleu = compute_bleu(model, src_val, tgt_val, batch_size=BATCH_SIZE, max_len=MAX_LEN)\n",
        "print(f\"Validation BLEU score: {val_bleu:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYQuT_vabLB2",
        "outputId": "a375d16b-2b95-48bc-b97a-5c6c3696d134"
      },
      "execution_count": 299,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation BLEU score: 0.0480\n"
          ]
        }
      ]
    }
  ]
}